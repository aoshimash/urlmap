# Crawld

A web crawler daemon for collecting and processing web content.

## ğŸš€ Features

- CLI-based web crawler
- HTML parsing and content extraction
- HTTP client with retry capabilities
- Modular architecture for extensibility

## ğŸ“‹ Project Structure

```
crawld/
â”œâ”€â”€ cmd/                # Command-line applications
â”‚   â””â”€â”€ crawld/         # Main CLI application
â”œâ”€â”€ internal/           # Private application code
â”‚   â””â”€â”€ crawler/        # Core crawler logic
â”œâ”€â”€ pkg/               # Public library code
â”‚   â””â”€â”€ utils/         # Utility functions
â”œâ”€â”€ go.mod             # Go module file
â”œâ”€â”€ go.sum             # Go dependency checksums
â””â”€â”€ README.md          # This file
```

## ğŸ›  Installation

```bash
# Clone the repository
git clone https://github.com/aoshimash/crawld.git
cd crawld

# Build the application
go build -o bin/crawld ./cmd/crawld

# Run the application
./bin/crawld
```

## ğŸ¯ Usage

```bash
# Basic usage
crawld --help

# Example usage (to be implemented)
crawld crawl https://example.com
```

## ğŸ§ª Development

### Prerequisites

- Go 1.21 or higher

### Build

```bash
go build ./...
```

### Test

```bash
go test ./...
```

### Lint

```bash
go vet ./...
```

## ğŸ“š Dependencies

- [Cobra](https://github.com/spf13/cobra) - CLI framework
- [Resty](https://github.com/go-resty/resty) - HTTP client library
- [goquery](https://github.com/PuerkitoBio/goquery) - HTML parsing

## ğŸ“ License

See [LICENSE](LICENSE) file for details.
A fast recursive web crawler for extracting all URLs from websites.
