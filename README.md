# Crawld

[![CI](https://github.com/aoshimash/crawld/workflows/CI/badge.svg)](https://github.com/aoshimash/crawld/actions/workflows/ci.yml)
[![Docker](https://github.com/aoshimash/crawld/workflows/Docker%20Build%20and%20Publish/badge.svg)](https://github.com/aoshimash/crawld/actions/workflows/docker.yml)

A web crawler daemon for collecting and processing web content.

## ğŸš€ Features

- CLI-based web crawler
- HTML parsing and content extraction
- HTTP client with retry capabilities
- Modular architecture for extensibility

## ğŸ“‹ Project Structure

```
crawld/
â”œâ”€â”€ cmd/                # Command-line applications
â”‚   â””â”€â”€ crawld/         # Main CLI application
â”œâ”€â”€ internal/           # Private application code
â”‚   â””â”€â”€ crawler/        # Core crawler logic
â”œâ”€â”€ pkg/               # Public library code
â”‚   â””â”€â”€ utils/         # Utility functions
â”œâ”€â”€ go.mod             # Go module file
â”œâ”€â”€ go.sum             # Go dependency checksums
â””â”€â”€ README.md          # This file
```

## ğŸ›  Installation

```bash
# Clone the repository
git clone https://github.com/aoshimash/crawld.git
cd crawld

# Build the application
go build -o bin/crawld ./cmd/crawld

# Run the application
./bin/crawld
```

## ğŸ¯ Usage

### Binary Usage

```bash
# Basic usage
crawld --help

# Crawl a website
crawld https://example.com

# Advanced usage with options
crawld -d 3 -c 5 --verbose https://example.com
```

### Docker Usage

```bash
# Pull from GitHub Container Registry
docker pull ghcr.io/aoshimash/crawld:latest

# Run with Docker
docker run --rm ghcr.io/aoshimash/crawld:latest --help

# Crawl a website using Docker
docker run --rm ghcr.io/aoshimash/crawld:latest https://example.com

# Advanced usage with Docker
docker run --rm ghcr.io/aoshimash/crawld:latest -d 3 -c 5 --verbose https://example.com
```

## ğŸ§ª Development

### Prerequisites

- Go 1.21 or higher

### Continuous Integration

This project uses GitHub Actions for continuous integration. The CI workflow:

- Tests on Go 1.21 and 1.22
- Runs `go fmt`, `go vet`, and `go test`
- Includes dependency caching for faster builds
- Generates code coverage reports

### Build

```bash
go build ./...
```

### Test

```bash
go test ./...
```

### Lint

```bash
go vet ./...
```

## ğŸ“š Dependencies

- [Cobra](https://github.com/spf13/cobra) - CLI framework
- [Resty](https://github.com/go-resty/resty) - HTTP client library
- [goquery](https://github.com/PuerkitoBio/goquery) - HTML parsing

## ğŸ“ License

See [LICENSE](LICENSE) file for details.
A fast recursive web crawler for extracting all URLs from websites.
